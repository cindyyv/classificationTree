{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934b83ed"
      },
      "source": [
        "#DATASET 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab7c0d5d",
        "outputId": "3cd6b667-adcb-42f5-df69-d4aebc1ec3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "# packages\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing import sequence\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import csv files to Google Colab\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "JN7AfIwQbyio"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "102f456d"
      },
      "outputs": [],
      "source": [
        "# Read csv file and rawdata in dataframe type\n",
        "def read_file(path):\n",
        "    rawdata = pd.read_csv(path, header=0, delimiter='\\t')\n",
        "    return rawdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "351d037d"
      },
      "outputs": [],
      "source": [
        "# Preprocess data before training\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "def preprocess_data(df):\n",
        "    reviews = []\n",
        "    for raw in tqdm(df['Phrase']):\n",
        "        # print(raw)\n",
        "        text = raw.replace(\"'\",\"\")\n",
        "        tokenized_train_data = text_to_word_sequence(text,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=\" \")\n",
        "        sw = set(stopwords.words('english'))\n",
        "        removesw = [i for i in tokenized_train_data if not i in sw]\n",
        "        rswtext = ' '.join(removesw)\n",
        "        numberRemove = ''.join(num for num in rswtext if not num.isdigit())\n",
        "        stemmer = PorterStemmer()\n",
        "        stem_input = nltk.word_tokenize(numberRemove)\n",
        "        stem_text = ' '.join([stemmer.stem(word) for word in stem_input])\n",
        "        reviews.append(stem_text)\n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "6fb102e7"
      },
      "outputs": [],
      "source": [
        "# Tokenizer preprocess before training\n",
        "def tokenizer_preprocess(list_X_train, list_X_val):\n",
        "    unique_words = set()\n",
        "    len_max = 0\n",
        "    for sent in tqdm(list_X_train):\n",
        "        unique_words.update(sent)\n",
        "        if len_max < len(sent):\n",
        "            len_max = len(sent)\n",
        "    len(list(unique_words)), len_max\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
        "    tokenizer.fit_on_texts(list(list_X_train))\n",
        "     \n",
        "    X_train = tokenizer.texts_to_sequences(list_X_train)\n",
        "    X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
        "\n",
        "    X_val = tokenizer.texts_to_sequences(list_X_val)\n",
        "    X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
        "\n",
        "    return X_train, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "2eca902a"
      },
      "outputs": [],
      "source": [
        "# Show the accuracy and other matrices of accuracy\n",
        "def report(predictions, y_test):\n",
        "    print('Accuracy: %s' % accuracy_score(y_test, predictions))\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# functions used to construct the decision tree classifier\n",
        "def gini(sequence, weights=None):\n",
        "    if weights is None: # count 1 - sum of square of probabilities\n",
        "        _, counts = np.unique(sequence, return_counts=True)\n",
        "        p = (counts / len(sequence)) ** 2  \n",
        "        return 1.0 - np.sum(p)\n",
        "    else:\n",
        "        tot = 0\n",
        "        weights = weights / weights.sum()\n",
        "        for c in np.unique(sequence):\n",
        "            # change prob become weighted prob\n",
        "            tot = np.sum(weights[sequence == c]) ** 2\n",
        "        return 1 - tot\n",
        "\n",
        "\n",
        "def entropy(sequence, weights=None):\n",
        "    if weights is None:\n",
        "        _, counts = np.unique(sequence, return_counts=True)\n",
        "        p = counts / len(sequence)  # probability\n",
        "        return -np.sum(p * np.log2(p))\n",
        "    else:\n",
        "        entropy = 0\n",
        "        weights = weights / weights.sum()\n",
        "        for c in np.unique(sequence):\n",
        "            # the weighted probability\n",
        "            tmp = np.sum(weights[sequence == c])\n",
        "            entropy -= tmp * np.log2(tmp)\n",
        "        return entropy"
      ],
      "metadata": {
        "id": "1jZMqYdUHmPi"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Node\n",
        "class Node():\n",
        "    def __init__(self, c_value, prediction):\n",
        "        self.c_value = c_value\n",
        "        self.prediction = prediction\n",
        "        self.feature_idx = None\n",
        "        self.threshold = None\n",
        "        self.left = None\n",
        "        self.right = None"
      ],
      "metadata": {
        "id": "nEQtT8opDJuK"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree built from scratch using gini and entropy\n",
        "class DecisionTree():\n",
        "    def __init__(self, criterion='gini', max_depth=None, max_features=None):\n",
        "        self.criterion = globals()[criterion]\n",
        "\n",
        "        self.max_depth = max_depth if max_depth is not None else 2 ** 100\n",
        "        self.max_features = max_features\n",
        "        self.n_features = None\n",
        "\n",
        "    def fit(self, x_data, y_data, sample_weight=None):\n",
        "        self.n_features = x_data.shape[1]\n",
        "        self.root = self.get_node(x_data, y_data, depth=0, sample_weight=sample_weight)\n",
        "\n",
        "    def get_node(self, x, y, depth, sample_weight=None):\n",
        "        weighted_counts = np.bincount(y, weights=sample_weight)\n",
        "        prediction = np.argmax(weighted_counts)\n",
        "            \n",
        "        node = Node(c_value=self.criterion(y, sample_weight), prediction=prediction)\n",
        "        if depth >= self.max_depth:\n",
        "            return node\n",
        "\n",
        "        node.feature_idx, node.threshold = self.best_split(x, y, sample_weight)\n",
        "        if node.feature_idx is None:\n",
        "            return node\n",
        "\n",
        "        left_idx = x[:, node.feature_idx] < node.threshold\n",
        "        x_left, y_left = x[left_idx], y[left_idx]\n",
        "        x_right, y_right = x[~left_idx], y[~left_idx]\n",
        "\n",
        "        # get child nodes recursively\n",
        "        if sample_weight is not None:\n",
        "            node.left = self.get_node(x_left, y_left, depth=depth + 1, sample_weight=sample_weight[left_idx])\n",
        "            node.right = self.get_node(x_right, y_right, depth=depth + 1, sample_weight=sample_weight[~left_idx])\n",
        "        else:\n",
        "            node.left = self.get_node(x_left, y_left, depth=depth + 1)\n",
        "            node.right = self.get_node(x_right, y_right, depth=depth + 1)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def best_split(self, x, y, sample_weight):\n",
        "        if len(y) <= 1:  \n",
        "            return None, None\n",
        "\n",
        "        parent_c = self.criterion(y, sample_weight)\n",
        "        best_infog = -2 ** 64  \n",
        "        best_idx, best_th = None, None\n",
        "\n",
        "        if self.max_features is not None:\n",
        "            available_features = np.random.choice(np.arange(self.n_features), size=self.max_features, replace=False)\n",
        "        else:\n",
        "            available_features = np.arange(self.n_features)\n",
        "\n",
        "        for idx in available_features:\n",
        "            sort_idx = np.argsort(x[:, idx])\n",
        "            thresholds = x[sort_idx, idx]\n",
        "            labels = y[sort_idx]\n",
        "\n",
        "            for pos in range(1, len(y)):\n",
        "                if thresholds[pos] == thresholds[pos - 1]:\n",
        "                    continue\n",
        "\n",
        "                if sample_weight is not None:\n",
        "                    sorted_sample_weight = sample_weight[sort_idx]\n",
        "                    left_c = self.criterion(\n",
        "                        labels[:pos], sorted_sample_weight[:pos])\n",
        "                    right_c = self.criterion(\n",
        "                        labels[pos:], sorted_sample_weight[pos:])\n",
        "                else:\n",
        "                    left_c = self.criterion(labels[:pos])\n",
        "                    right_c = self.criterion(labels[pos:])\n",
        "\n",
        "                child_c = (pos * left_c + (len(y) - pos) * right_c) / len(y)\n",
        "                infog = parent_c - child_c\n",
        "\n",
        "                if infog > best_infog:\n",
        "                    best_infog = infog\n",
        "                    best_idx = idx\n",
        "                    best_th = (thresholds[pos] + thresholds[pos - 1]) / 2\n",
        "\n",
        "        return best_idx, best_th\n",
        "\n",
        "    def predict(self, x_data):\n",
        "        def util(self, x):\n",
        "            cur_node = self.root\n",
        "            while cur_node.left and cur_node.right:\n",
        "                if x[cur_node.feature_idx] < cur_node.threshold:\n",
        "                    cur_node = cur_node.left\n",
        "                else:\n",
        "                    cur_node = cur_node.right\n",
        "                    \n",
        "            return cur_node.prediction\n",
        "        return np.stack([util(self, single_x) for single_x in x_data])"
      ],
      "metadata": {
        "id": "EjV07Or3Hj_h"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN CODE"
      ],
      "metadata": {
        "id": "jxRZg8RUryqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "30dbc21c"
      },
      "outputs": [],
      "source": [
        "# Read the csv files inserted\n",
        "df = read_file('./X_train.csv')\n",
        "df2 = read_file('./y_train.csv')\n",
        "df3 = read_file('./X_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---> TRAIN"
      ],
      "metadata": {
        "id": "ZA-9ngLwsez1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31b5c00c",
        "outputId": "0bd26fd7-75d5-49ed-8f9d-14de10f3a562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 124848/124848 [01:00<00:00, 2070.20it/s]\n",
            "100%|██████████| 31211/31211 [00:14<00:00, 2113.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# preprocess training and testing data\n",
        "train_text = preprocess_data(df)\n",
        "test_text = preprocess_data(df3)\n",
        "target = df2.Sentiment.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split validation from training dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_text, target, test_size=0.2, stratify=target)"
      ],
      "metadata": {
        "id": "Y4Vd3qCO2Hyu"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save preprocessed training data to a new csv file\n",
        "final_traindf = pd.DataFrame(train_text) \n",
        "final_traindf.to_csv('X_train_final.csv') "
      ],
      "metadata": {
        "id": "Gg5zeGL-cOjr"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save preprocessed testing data to a new csv file\n",
        "final_traindf = pd.DataFrame(test_text) \n",
        "final_traindf.to_csv('X_test_final.csv') "
      ],
      "metadata": {
        "id": "S9arKjtps21e"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f12ac0e",
        "outputId": "b5e1c076-9bdf-4d77-e849-06ce25f24251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99878/99878 [00:00<00:00, 811100.28it/s]\n"
          ]
        }
      ],
      "source": [
        "# tokenize the training and validation data\n",
        "X_train_, X_val_ = tokenizer_preprocess(X_train, X_val) \n",
        "# X_train_, X_val_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init decision tree classifier\n",
        "clf = DecisionTree(criterion='entropy')\n",
        "clf.fit(X_train_, y_train)"
      ],
      "metadata": {
        "id": "_9Y8ogRaQAnx"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict validation set\n",
        "y_pred = clf.predict(X_val_)\n",
        "report(y_pred, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z58dfuOGsnqq",
        "outputId": "8c8fc6ed-651a-4a4f-f03a-ef8ab37cf321"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5243091710052062\n",
            "Confusion Matrix:\n",
            "[[   36    79   969    36    12]\n",
            " [   42   223  3935   128    36]\n",
            " [   11   129 12284   239    70]\n",
            " [    6    88  4627   444   103]\n",
            " [    7    32  1148   181   105]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.03      0.06      1132\n",
            "           1       0.40      0.05      0.09      4364\n",
            "           2       0.53      0.96      0.69     12733\n",
            "           3       0.43      0.08      0.14      5268\n",
            "           4       0.32      0.07      0.12      1473\n",
            "\n",
            "    accuracy                           0.52     24970\n",
            "   macro avg       0.41      0.24      0.22     24970\n",
            "weighted avg       0.47      0.52      0.41     24970\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---> TEST"
      ],
      "metadata": {
        "id": "a9D0WQm-s_qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize before testing\n",
        "X_train_, X_test_ = tokenizer_preprocess(X_train, test_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LP3hAIzesne",
        "outputId": "7e7324e8-5014-40d2-ac3f-6d70d1c0bbc8"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99878/99878 [00:00<00:00, 506128.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict testing dataset\n",
        "ytest_pred = clf.predict(X_test_)"
      ],
      "metadata": {
        "id": "qOrdEaR_ebcN"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write numpy array to csv\n",
        "np.savetxt(\"y_test_final.csv\", ytest_pred, delimiter=\",\")"
      ],
      "metadata": {
        "id": "ckt_ilavqW_i"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference of sklearn package DecisionTreeClassifier\n",
        "# model = DecisionTreeClassifier()\n",
        "# model.fit(X_train_, y_train)\n",
        "# predictions = model.predict(X_val_)\n",
        "# report(predictions, y_val)"
      ],
      "metadata": {
        "id": "20bM79l8h068"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finpredictions = model.predict(X_test_)"
      ],
      "metadata": {
        "id": "xuz3-cC4h8jE"
      },
      "execution_count": 204,
      "outputs": []
    }
  ]
}